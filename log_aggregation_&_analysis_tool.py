# -*- coding: utf-8 -*-
"""Log Aggregation & Analysis Tool

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y1sEyvWd3UiDWmKNrdVAn4E3hqiXSSDK
"""

import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, timedelta
import random
import re
from collections import Counter, defaultdict

class LogAnalyzer:
    def __init__(self):
        self.logs = []
        self.patterns = defaultdict(int)
        self.anomalies = []
        self.error_trends = defaultdict(list)

    def generate_sample_logs(self, num_logs=5000):
        """Generate realistic log samples"""
        log_levels = ['INFO', 'DEBUG', 'WARNING', 'ERROR', 'CRITICAL']
        level_weights = [0.50, 0.25, 0.15, 0.08, 0.02]

        services = ['auth-service', 'api-gateway', 'database', 'cache', 'queue-worker']

        messages = {
            'INFO': [
                'Request processed successfully',
                'User login successful',
                'Cache hit',
                'Database query executed',
                'Message sent to queue'
            ],
            'DEBUG': [
                'Entering function process_request',
                'Variable state: active',
                'Cache lookup performed',
                'Query execution time: {}ms'
            ],
            'WARNING': [
                'High memory usage detected',
                'Slow query performance',
                'Connection pool near capacity',
                'Rate limit approaching'
            ],
            'ERROR': [
                'Database connection failed',
                'Authentication timeout',
                'Invalid request format',
                'Service unavailable',
                'Failed to parse JSON'
            ],
            'CRITICAL': [
                'System out of memory',
                'Database crashed',
                'Security breach detected',
                'Data corruption found'
            ]
        }

        start_time = datetime.now() - timedelta(hours=24)

        for i in range(num_logs):
            timestamp = start_time + timedelta(seconds=random.randint(0, 86400))
            level = random.choices(log_levels, weights=level_weights)[0]
            service = random.choice(services)
            message = random.choice(messages[level])

            # Add response time for some messages
            if 'query' in message.lower() or 'request' in message.lower():
                response_time = random.randint(10, 500)
                message = f"{message} (response_time={response_time}ms)"

            # Create anomalies - spike in errors at certain times
            if 10 <= timestamp.hour <= 11 and random.random() < 0.3:
                level = 'ERROR'
                message = random.choice(messages['ERROR'])

            log_entry = {
                'timestamp': timestamp,
                'level': level,
                'service': service,
                'message': message,
                'user_id': f"user_{random.randint(1000, 9999)}",
                'request_id': f"req_{random.randint(100000, 999999)}"
            }

            self.logs.append(log_entry)

        self.logs.sort(key=lambda x: x['timestamp'])

    def analyze_patterns(self):
        """Analyze log patterns"""
        for log in self.logs:
            # Extract patterns
            pattern = re.sub(r'\d+', 'N', log['message'])  # Replace numbers with N
            pattern = re.sub(r'user_\d+', 'user_X', pattern)  # Replace user IDs
            self.patterns[pattern] += 1

    def detect_anomalies(self):
        """Detect anomalies in log patterns"""
        # Group logs by hour
        hourly_errors = defaultdict(int)

        for log in self.logs:
            hour = log['timestamp'].replace(minute=0, second=0, microsecond=0)
            if log['level'] in ['ERROR', 'CRITICAL']:
                hourly_errors[hour] += 1
                self.error_trends[hour].append(log)

        # Calculate mean and std
        error_counts = list(hourly_errors.values())
        mean_errors = np.mean(error_counts)
        std_errors = np.std(error_counts)

        # Detect anomalies (> 2 standard deviations)
        threshold = mean_errors + (2 * std_errors)

        for hour, count in hourly_errors.items():
            if count > threshold:
                self.anomalies.append({
                    'timestamp': hour,
                    'error_count': count,
                    'threshold': threshold,
                    'severity': 'high' if count > threshold * 1.5 else 'medium'
                })

    def generate_dashboard(self):
        """Generate comprehensive log analysis dashboard"""
        fig = plt.figure(figsize=(18, 12))
        fig.suptitle('Log Analysis & Anomaly Detection Dashboard',
                     fontsize=20, fontweight='bold')

        # 1. Log Level Distribution
        ax1 = plt.subplot(3, 3, 1)
        level_counts = Counter([log['level'] for log in self.logs])
        levels = list(level_counts.keys())
        counts = list(level_counts.values())

        colors_map = {
            'INFO': '#3498db',
            'DEBUG': '#95a5a6',
            'WARNING': '#f39c12',
            'ERROR': '#e74c3c',
            'CRITICAL': '#c0392b'
        }
        colors = [colors_map.get(l, '#95a5a6') for l in levels]

        wedges, texts, autotexts = ax1.pie(counts, labels=levels, autopct='%1.1f%%',
                                            colors=colors, startangle=90)
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontweight('bold')
        ax1.set_title('Log Level Distribution', fontweight='bold')

        # 2. Logs Over Time
        ax2 = plt.subplot(3, 3, 2)
        time_buckets = defaultdict(int)

        for log in self.logs:
            bucket = log['timestamp'].replace(minute=0, second=0, microsecond=0)
            time_buckets[bucket] += 1

        times = sorted(time_buckets.keys())
        log_counts = [time_buckets[t] for t in times]

        ax2.plot(times, log_counts, linewidth=2, color='#3498db', marker='o', markersize=4)
        ax2.fill_between(times, log_counts, alpha=0.3, color='#3498db')
        ax2.set_ylabel('Log Count', fontweight='bold')
        ax2.set_title('Log Volume Over Time', fontweight='bold')
        ax2.grid(True, alpha=0.3)
        plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45)

        # 3. Error Rate Trends
        ax3 = plt.subplot(3, 3, 3)
        error_buckets = defaultdict(int)
        total_buckets = defaultdict(int)

        for log in self.logs:
            bucket = log['timestamp'].replace(minute=0, second=0, microsecond=0)
            total_buckets[bucket] += 1
            if log['level'] in ['ERROR', 'CRITICAL']:
                error_buckets[bucket] += 1

        times = sorted(total_buckets.keys())
        error_rates = [(error_buckets[t] / total_buckets[t]) * 100 for t in times]

        ax3.plot(times, error_rates, linewidth=2, color='#e74c3c', marker='s', markersize=4)
        ax3.fill_between(times, error_rates, alpha=0.3, color='#e74c3c')
        ax3.set_ylabel('Error Rate (%)', fontweight='bold')
        ax3.set_title('Error Rate Trends', fontweight='bold')
        ax3.axhline(y=np.mean(error_rates), color='orange', linestyle='--',
                   label=f'Avg: {np.mean(error_rates):.2f}%')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        plt.setp(ax3.xaxis.get_majorticklabels(), rotation=45)

        # 4. Top Error Patterns
        ax4 = plt.subplot(3, 3, 4)
        error_patterns = Counter()
        for log in self.logs:
            if log['level'] in ['ERROR', 'CRITICAL']:
                pattern = re.sub(r'\d+', 'N', log['message'])
                error_patterns[pattern] += 1

        top_patterns = error_patterns.most_common(8)
        if top_patterns:
            patterns_text = [p[0][:30] + '...' if len(p[0]) > 30 else p[0]
                           for p in top_patterns]
            pattern_counts = [p[1] for p in top_patterns]

            bars = ax4.barh(range(len(patterns_text)), pattern_counts,
                           color='#e74c3c', alpha=0.7)
            ax4.set_yticks(range(len(patterns_text)))
            ax4.set_yticklabels(patterns_text, fontsize=8)
            ax4.set_xlabel('Occurrences', fontweight='bold')
            ax4.set_title('Top Error Patterns', fontweight='bold')
            ax4.grid(axis='x', alpha=0.3)

            for i, bar in enumerate(bars):
                width = bar.get_width()
                ax4.text(width, bar.get_y() + bar.get_height()/2,
                        f' {int(width)}', ha='left', va='center', fontweight='bold')

        # 5. Service-wise Log Distribution
        ax5 = plt.subplot(3, 3, 5)
        service_counts = Counter([log['service'] for log in self.logs])
        services = list(service_counts.keys())
        service_log_counts = list(service_counts.values())

        ax5.bar(services, service_log_counts, color='#9b59b6', alpha=0.7, edgecolor='black')
        ax5.set_ylabel('Log Count', fontweight='bold')
        ax5.set_title('Logs by Service', fontweight='bold')
        ax5.grid(axis='y', alpha=0.3)
        plt.setp(ax5.xaxis.get_majorticklabels(), rotation=45, ha='right')

        # 6. Anomaly Detection
        ax6 = plt.subplot(3, 3, 6)
        hourly_errors = defaultdict(int)
        for log in self.logs:
            hour = log['timestamp'].replace(minute=0, second=0, microsecond=0)
            if log['level'] in ['ERROR', 'CRITICAL']:
                hourly_errors[hour] += 1

        times = sorted(hourly_errors.keys())
        error_counts = [hourly_errors[t] for t in times]

        mean_errors = np.mean(error_counts)
        std_errors = np.std(error_counts)
        threshold = mean_errors + (2 * std_errors)

        ax6.plot(times, error_counts, linewidth=2, color='#e74c3c',
                marker='o', markersize=6, label='Errors')
        ax6.axhline(y=threshold, color='red', linestyle='--', linewidth=2,
                   label=f'Threshold: {threshold:.1f}')
        ax6.axhline(y=mean_errors, color='green', linestyle='--', linewidth=2,
                   label=f'Mean: {mean_errors:.1f}')

        # Mark anomalies
        for anomaly in self.anomalies:
            ax6.scatter(anomaly['timestamp'], anomaly['error_count'],
                       s=200, color='red', marker='*', zorder=5)

        ax6.set_ylabel('Error Count', fontweight='bold')
        ax6.set_title(f'Anomaly Detection ({len(self.anomalies)} anomalies)',
                     fontweight='bold')
        ax6.legend()
        ax6.grid(True, alpha=0.3)
        plt.setp(ax6.xaxis.get_majorticklabels(), rotation=45)

        # 7. Response Time Distribution
        ax7 = plt.subplot(3, 3, 7)
        response_times = []
        for log in self.logs:
            match = re.search(r'response_time=(\d+)ms', log['message'])
            if match:
                response_times.append(int(match.group(1)))

        if response_times:
            ax7.hist(response_times, bins=50, color='#1abc9c', alpha=0.7, edgecolor='black')
            ax7.axvline(x=np.median(response_times), color='red', linestyle='--',
                       linewidth=2, label=f'Median: {np.median(response_times):.0f}ms')
            ax7.axvline(x=np.percentile(response_times, 95), color='orange',
                       linestyle='--', linewidth=2,
                       label=f'P95: {np.percentile(response_times, 95):.0f}ms')
            ax7.set_xlabel('Response Time (ms)', fontweight='bold')
            ax7.set_ylabel('Frequency', fontweight='bold')
            ax7.set_title('Response Time Distribution', fontweight='bold')
            ax7.legend()
            ax7.grid(axis='y', alpha=0.3)

        # 8. Error Severity Heatmap
        ax8 = plt.subplot(3, 3, 8)
        services = ['auth-service', 'api-gateway', 'database', 'cache', 'queue-worker']
        error_levels = ['WARNING', 'ERROR', 'CRITICAL']

        heatmap_data = np.zeros((len(services), len(error_levels)))

        for log in self.logs:
            if log['level'] in error_levels:
                service_idx = services.index(log['service'])
                level_idx = error_levels.index(log['level'])
                heatmap_data[service_idx][level_idx] += 1

        im = ax8.imshow(heatmap_data, cmap='YlOrRd', aspect='auto')
        ax8.set_xticks(np.arange(len(error_levels)))
        ax8.set_yticks(np.arange(len(services)))
        ax8.set_xticklabels(error_levels)
        ax8.set_yticklabels(services)
        ax8.set_title('Error Severity by Service', fontweight='bold')

        for i in range(len(services)):
            for j in range(len(error_levels)):
                text = ax8.text(j, i, int(heatmap_data[i, j]),
                              ha="center", va="center", color="black", fontweight='bold')

        plt.colorbar(im, ax=ax8, label='Count')

        # 9. Log Activity Timeline
        ax9 = plt.subplot(3, 3, 9)
        hours = [log['timestamp'].hour for log in self.logs]
        hour_counts = Counter(hours)

        all_hours = list(range(24))
        counts_by_hour = [hour_counts.get(h, 0) for h in all_hours]

        colors_timeline = ['#e74c3c' if c > np.mean(counts_by_hour) * 1.5
                          else '#3498db' for c in counts_by_hour]

        ax9.bar(all_hours, counts_by_hour, color=colors_timeline, alpha=0.7, edgecolor='black')
        ax9.set_xlabel('Hour of Day', fontweight='bold')
        ax9.set_ylabel('Log Count', fontweight='bold')
        ax9.set_title('Hourly Log Activity Pattern', fontweight='bold')
        ax9.grid(axis='y', alpha=0.3)
        ax9.set_xticks(range(0, 24, 2))

        plt.tight_layout()
        plt.savefig('log_analysis_dashboard.png', dpi=300, bbox_inches='tight')
        print("‚úÖ Dashboard saved as 'log_analysis_dashboard.png'")
        plt.show()

    def generate_report(self):
        """Generate text report"""
        print("\n" + "="*80)
        print(" "*25 + "LOG ANALYSIS REPORT")
        print("="*80 + "\n")

        total_logs = len(self.logs)
        level_counts = Counter([log['level'] for log in self.logs])

        print(f"üìä Total Logs Analyzed: {total_logs:,}")
        print(f"\nüìà Log Level Breakdown:")
        for level in ['INFO', 'DEBUG', 'WARNING', 'ERROR', 'CRITICAL']:
            count = level_counts.get(level, 0)
            percentage = (count / total_logs) * 100
            print(f"   {level:10s}: {count:6,} ({percentage:5.2f}%)")

        error_count = level_counts.get('ERROR', 0) + level_counts.get('CRITICAL', 0)
        error_rate = (error_count / total_logs) * 100

        print(f"\n‚ö†Ô∏è  Error Rate: {error_rate:.2f}%")
        print(f"üî¥ Critical Errors: {level_counts.get('CRITICAL', 0)}")
        print(f"üîç Anomalies Detected: {len(self.anomalies)}")

        if self.anomalies:
            print(f"\nüö® Anomaly Details:")
            for anomaly in self.anomalies[:3]:
                print(f"   {anomaly['timestamp'].strftime('%Y-%m-%d %H:%M')} - "
                      f"{anomaly['error_count']} errors (threshold: {anomaly['threshold']:.0f})")

        print("\n" + "="*80 + "\n")

# Main execution
if __name__ == "__main__":
    print("üöÄ Starting Log Analyzer...")

    analyzer = LogAnalyzer()
    print("üìù Generating sample logs...")
    analyzer.generate_sample_logs(num_logs=5000)

    print("üîç Analyzing patterns...")
    analyzer.analyze_patterns()

    print("üéØ Detecting anomalies...")
    analyzer.detect_anomalies()

    print("üìä Generating dashboard...")
    analyzer.generate_dashboard()

    analyzer.generate_report()

    print("‚úÖ Analysis complete!")